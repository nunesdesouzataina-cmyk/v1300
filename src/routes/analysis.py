#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v4.0 - Sistema de An√°lise ULTRA ROBUSTO
Implementa√ß√£o final combinando as melhores pr√°ticas de todas as vers√µes
"""

import os
import logging
import time
import uuid
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, Callable, List
from flask import Blueprint, request, jsonify
from concurrent.futures import ThreadPoolExecutor

# Servi√ßos especializados
from services import (
    master_analysis_orchestrator,
    real_search_orchestrator,
    viral_content_analyzer,
    enhanced_synthesis_engine,
    enhanced_module_processor,
    comprehensive_report_generator_v3,
    auto_save_manager,
    progress_tracker_enhanced
)
from services.predictive_analytics_service import predictive_analytics_service as predictive_analytics_engine

logger = logging.getLogger(__name__)
analysis_bp = Blueprint('analysis', __name__)

class UltraRobustAnalysisManager:
    """Sistema central ULTRA ROBUSTO para gerenciamento de an√°lises completas"""
    
    def __init__(self):
        """Inicializa o sistema com valida√ß√£o completa de integridade"""
        self.base_path = Path("analyses_data")
        self.required_modules = self._define_required_modules()
        self._ensure_all_directories()
        self._validate_system_integrity()
        logger.info(f"‚úÖ Sistema ULTRA ROBUSTO inicializado com {len(self.required_modules)} m√≥dulos obrigat√≥rios")
    
    def _define_required_modules(self) -> Dict[str, Dict]:
        """Define todos os m√≥dulos obrigat√≥rios com suas prioridades e descri√ß√µes"""
        return {
            'analyses': {'priority': 1, 'required': True, 'description': 'An√°lises principais'},
            'avatars': {'priority': 2, 'required': True, 'description': 'Avatars ultra-detalhados'},
            'drivers_mentais': {'priority': 3, 'required': True, 'description': 'Drivers psicol√≥gicos'},
            'concorrencia': {'priority': 4, 'required': True, 'description': 'An√°lise competitiva'},
            'funil_vendas': {'priority': 5, 'required': True, 'description': 'Otimiza√ß√£o de funil'},
            'insights': {'priority': 6, 'required': True, 'description': 'Insights estrat√©gicos'},
            'metricas': {'priority': 7, 'required': True, 'description': 'M√©tricas e KPIs'},
            'palavras_chave': {'priority': 8, 'required': True, 'description': 'Keywords estrat√©gicas'},
            'pesquisa_web': {'priority': 9, 'required': True, 'description': 'Dados web reais'},
            'plano_acao': {'priority': 10, 'required': True, 'description': 'Planos acion√°veis'},
            'posicionamento': {'priority': 11, 'required': True, 'description': 'Posicionamento estrat√©gico'},
            'predicoes_futuro': {'priority': 12, 'required': True, 'description': 'Predi√ß√µes baseadas em dados'},
            'provas_visuais': {'priority': 13, 'required': True, 'description': 'Evid√™ncias visuais'},
            'anti_objecao': {'priority': 14, 'required': True, 'description': 'Sistema anti-obje√ß√£o'},
            'pre_pitch': {'priority': 15, 'required': True, 'description': 'Pr√©-pitch estrat√©gico'},
            'reports': {'priority': 16, 'required': True, 'description': 'Relat√≥rios finais'},
            'users': {'priority': 17, 'required': False, 'description': 'Dados de usu√°rios'},
            'files': {'priority': 18, 'required': False, 'description': 'Arquivos auxiliares'},
            'logs': {'priority': 19, 'required': False, 'description': 'Logs do sistema'},
            'metadata': {'priority': 20, 'required': False, 'description': 'Metadados'},
            'progress': {'priority': 21, 'required': False, 'description': 'Progresso das an√°lises'}
        }
    
    def _ensure_all_directories(self):
        """Garante que todas as pastas necess√°rias existam"""
        for module_name in self.required_modules.keys():
            module_path = self.base_path / module_name
            module_path.mkdir(parents=True, exist_ok=True)
            
            # Cria arquivo de √≠ndice se n√£o existir
            index_file = module_path / "index.json"
            if not index_file.exists():
                index_data = {
                    "module": module_name,
                    "description": self.required_modules[module_name]['description'],
                    "created_at": datetime.now().isoformat(),
                    "sessions": [],
                    "total_analyses": 0
                }
                with open(index_file, 'w', encoding='utf-8') as f:
                    json.dump(index_data, f, ensure_ascii=False, indent=2)
    
    def _validate_system_integrity(self):
        """Valida integridade completa do sistema"""
        missing_modules = [
            module_name for module_name, module_info in self.required_modules.items()
            if module_info['required'] and not (self.base_path / module_name).exists()
        ]
        
        if missing_modules:
            logger.error(f"‚ùå M√≥dulos obrigat√≥rios ausentes: {missing_modules}")
            raise Exception(f"Sistema incompleto - M√≥dulos ausentes: {missing_modules}")
        
        logger.info("‚úÖ Integridade do sistema validada - Todos os m√≥dulos dispon√≠veis")
    
    def validate_real_data(self, data: Any, category: str) -> bool:
        """Valida se dados s√£o REAIS (n√£o simulados/fict√≠cios)"""
        if not data:
            return False
        
        # Converte para string para an√°lise
        data_str = str(data).lower()
        
        # Palavras que indicam dados fict√≠cios/simulados
        forbidden_terms = [
            'fict√≠cio', 'simulado', 'exemplo', 'placeholder', 'mock', 'fake',
            'lorem ipsum', 'teste', 'sample', 'demo', 'hipot√©tico',
            'imagin√°rio', 'inventado', 'artificial', 'gen√©rico'
        ]
        
        # Verifica se cont√©m termos proibidos
        for term in forbidden_terms:
            if term in data_str:
                return False
        
        # Valida√ß√µes espec√≠ficas por categoria
        if category == 'pesquisa_web':
            # Deve ter URLs reais e conte√∫do extra√≠do
            if isinstance(data, dict):
                return bool(data.get('total_resultados', 0) > 0 and data.get('extracted_content'))
        
        elif category == 'avatars':
            # Deve ter dados demogr√°ficos espec√≠ficos
            if isinstance(data, dict):
                return bool(data.get('perfil_demografico') and data.get('dores_principais'))
        
        elif category == 'metricas':
            # Deve ter n√∫meros e m√©tricas reais
            return any(char.isdigit() for char in data_str)
        
        return True
    
    def calculate_data_quality(self, data: Any, category: str) -> float:
        """Calcula pontua√ß√£o de qualidade dos dados (0-100)"""
        base_score = 60.0  # Pontua√ß√£o base
        
        if not data:
            return 0.0
        
        data_str = str(data)
        
        # Pontua√ß√£o por tamanho do conte√∫do
        length_score = min(len(data_str) / 1000 * 20, 20)  # M√°ximo 20 pontos
        
        # Pontua√ß√£o por estrutura
        structure_score = 0
        if isinstance(data, dict):
            structure_score += 10
            if len(data.keys()) > 5:
                structure_score += 5
        elif isinstance(data, list):
            structure_score += 5
            if len(data) > 3:
                structure_score += 5
        
        # Pontua√ß√£o por presen√ßa de n√∫meros/dados
        numbers_count = sum(1 for char in data_str if char.isdigit())
        numbers_score = min(numbers_count / 50 * 10, 10)  # M√°ximo 10 pontos
        
        # Pontua√ß√£o espec√≠fica por categoria
        category_score = 0
        if category == 'pesquisa_web' and isinstance(data, dict):
            if data.get('total_resultados', 0) > 10:
                category_score += 5
            if data.get('extracted_content'):
                category_score += 5
        
        total_score = base_score + length_score + structure_score + numbers_score + category_score
        return min(total_score, 100.0)
    
    def get_comprehensive_session_summary(self, session_id: str) -> Dict[str, Any]:
        """Retorna resumo ULTRA COMPLETO de uma sess√£o"""
        summary = {
            'session_id': session_id,
            'analysis_timestamp': datetime.now().isoformat(),
            'system_validation': {
                'all_modules_available': True,
                'required_modules_count': len([m for m in self.required_modules.values() if m['required']]),
                'optional_modules_count': len([m for m in self.required_modules.values() if not m['required']])
            },
            'modules_analysis': {},
            'data_quality': {},
            'completeness_score': 0.0,
            'recommendations': []
        }
        
        total_priority_score = 0
        max_priority_score = 0
        
        for module_name, module_info in self.required_modules.items():
            module_path = self.base_path / module_name
            session_path = module_path / session_id
            
            module_analysis = {
                'priority': module_info['priority'],
                'required': module_info['required'],
                'description': module_info['description'],
                'has_data': False,
                'files_count': 0,
                'last_updated': None,
                'quality_score': 0.0
            }
            
            if session_path.exists():
                files = list(session_path.glob("*.json"))
                if files:
                    module_analysis['has_data'] = True
                    module_analysis['files_count'] = len(files)
                    
                    # Calcula qualidade m√©dia dos dados
                    quality_scores = []
                    for file_path in files:
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                file_data = json.load(f)
                            
                            data_validation = file_data.get('data_validation', {})
                            quality_score = data_validation.get('quality_score', 0)
                            quality_scores.append(quality_score)
                            
                            # √öltima atualiza√ß√£o
                            mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                            if not module_analysis['last_updated'] or mtime > module_analysis['last_updated']:
                                module_analysis['last_updated'] = mtime.isoformat()
                        
                        except Exception as e:
                            logger.warning(f"Erro ao processar {file_path}: {e}")
                            continue
                    
                    if quality_scores:
                        module_analysis['quality_score'] = sum(quality_scores) / len(quality_scores)
            
            # Calcula pontua√ß√£o de completude
            if module_info['required']:
                max_priority_score += (21 - module_info['priority'])  # Prioridade invertida
                if module_analysis['has_data']:
                    total_priority_score += (21 - module_info['priority'])
            
            summary['modules_analysis'][module_name] = module_analysis
        
        # Calcula pontua√ß√£o de completude geral
        if max_priority_score > 0:
            summary['completeness_score'] = (total_priority_score / max_priority_score) * 100
        
        # Gera recomenda√ß√µes
        summary['recommendations'] = self._generate_recommendations(summary['modules_analysis'])
        
        return summary
    
    def _generate_recommendations(self, modules_analysis: Dict) -> List[str]:
        """Gera recomenda√ß√µes baseadas na an√°lise dos m√≥dulos"""
        recommendations = []
        
        # Verifica m√≥dulos obrigat√≥rios faltantes
        missing_required = []
        low_quality = []
        
        for module_name, analysis in modules_analysis.items():
            if analysis['required'] and not analysis['has_data']:
                missing_required.append(module_name)
            elif analysis['has_data'] and analysis['quality_score'] < 70:
                low_quality.append(module_name)
        
        if missing_required:
            recommendations.append(f"CR√çTICO: M√≥dulos obrigat√≥rios ausentes: {', '.join(missing_required)}")
        
        if low_quality:
            recommendations.append(f"ATEN√á√ÉO: M√≥dulos com baixa qualidade (refazer): {', '.join(low_quality)}")
        
        # Recomenda√ß√µes de melhoria
        pesquisa_web = modules_analysis.get('pesquisa_web', {})
        if pesquisa_web.get('quality_score', 0) < 80:
            recommendations.append("Melhorar qualidade da pesquisa web - mais fontes e dados")
        
        avatars = modules_analysis.get('avatars', {})
        if not avatars.get('has_data'):
            recommendations.append("Criar avatars ultra-detalhados baseados em dados reais")
        
        return recommendations
    
    def ensure_all_modules_in_final_analysis(self, session_id: str) -> Dict[str, Any]:
        """GARANTE que TODOS os m√≥dulos sejam considerados na an√°lise final"""
        logger.info("üîç Verificando se TODOS os m√≥dulos est√£o presentes na an√°lise final...")
        
        final_analysis = {}
        missing_modules = []
        
        for module_name, module_info in self.required_modules.items():
            module_data = self.load_analysis_data(module_name, session_id)
            
            if module_data:
                final_analysis[module_name] = module_data
                logger.info(f"‚úÖ M√≥dulo {module_name} inclu√≠do na an√°lise final")
            else:
                if module_info['required']:
                    missing_modules.append(module_name)
                    logger.warning(f"‚ö†Ô∏è M√≥dulo obrigat√≥rio ausente: {module_name}")
                else:
                    logger.info(f"‚ÑπÔ∏è M√≥dulo opcional ausente: {module_name}")
        
        # Resumo final
        final_analysis['_meta'] = {
            'total_modules': len(self.required_modules),
            'included_modules': len(final_analysis) - 1,  # -1 para excluir _meta
            'missing_required_modules': missing_modules,
            'completeness_percentage': ((len(final_analysis) - 1) / len(self.required_modules)) * 100,
            'validation_timestamp': datetime.now().isoformat(),
            'all_modules_included': len(missing_modules) == 0
        }
        
        if missing_modules:
            logger.error(f"‚ùå AN√ÅLISE INCOMPLETA - M√≥dulos obrigat√≥rios ausentes: {missing_modules}")
        else:
            logger.info("‚úÖ AN√ÅLISE COMPLETA - Todos os m√≥dulos obrigat√≥rios inclu√≠dos")
        
        return final_analysis
    
    def load_analysis_data(self, category: str, session_id: str, filename: str = None) -> Optional[Any]:
        """Carrega dados de an√°lise com valida√ß√£o"""
        if category not in self.required_modules:
            logger.error(f"Categoria n√£o reconhecida: {category}")
            return None
        
        category_path = self.base_path / category
        session_path = category_path / session_id
        
        if not session_path.exists():
            logger.warning(f"Sess√£o n√£o encontrada: {session_id} em {category}")
            return None
        
        if filename:
            file_path = session_path / filename
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    loaded_data = json.load(f)
                return loaded_data.get('data', loaded_data)  # Retorna dados ou estrutura completa
        else:
            # Retorna arquivo mais recente
            files = list(session_path.glob(f"{category}_*.json"))
            if files:
                latest_file = max(files, key=os.path.getctime)
                with open(latest_file, 'r', encoding='utf-8') as f:
                    loaded_data = json.load(f)
                return loaded_data.get('data', loaded_data)
        
        return None

# Inst√¢ncia global ULTRA ROBUSTA
analysis_manager = UltraRobustAnalysisManager()

class LocalProgressTracker:
    """Rastreador de progresso local para evitar depend√™ncias externas e garantir integridade"""
    
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.current_step = 0
        self.total_steps = 13
        self.start_time = time.time()
        self.steps = [
            "üîç Validando dados de entrada e preparando an√°lise",
            "üåê Executando pesquisa web massiva com WebSailor",
            "üìÑ Extraindo conte√∫do de fontes preferenciais",
            "ü§ñ Analisando com Gemini 2.5 Pro (modelo prim√°rio)",
            "üë§ Criando avatar arqueol√≥gico ultra-detalhado",
            "üß† Gerando drivers mentais customizados (19 universais)",
            "üé≠ Desenvolvendo provas visuais instant√¢neas (PROVIs)",
            "üõ°Ô∏è Construindo sistema anti-obje√ß√£o psicol√≥gico",
            "üéØ Arquitetando pr√©-pitch invis√≠vel completo",
            "‚öîÔ∏è Mapeando concorr√™ncia e posicionamento",
            "üìà Calculando m√©tricas forenses e proje√ß√µes",
            "üîÆ Predizendo futuro do mercado (36 meses)",
            "‚ú® Consolidando an√°lise arqueol√≥gica final"
        ]
        self.detailed_logs = []
        self._save_to_disk()
    
    def update_progress(self, step: int, message: str, details: str = None):
        """Atualiza progresso localmente e salva no disco"""
        self.current_step = step
        current_time = time.time()
        elapsed = current_time - self.start_time
        
        log_entry = {
            "step": step,
            "message": message,
            "details": details,
            "timestamp": datetime.now().isoformat(),
            "elapsed": elapsed
        }
        self.detailed_logs.append(log_entry)
        
        logger.info(f"Progress {self.session_id}: Step {step}/{self.total_steps} - {message}")
        self._save_to_disk()
        
        return log_entry
    
    def complete(self):
        """Marca como completo e salva estado final"""
        self.update_progress(self.total_steps, "üéâ An√°lise conclu√≠da! Preparando resultados...")
        self._save_to_disk()
    
    def get_current_status(self):
        """Retorna status atual com informa√ß√µes detalhadas"""
        elapsed = time.time() - self.start_time
        return {
            "session_id": self.session_id,
            "current_step": self.current_step,
            "total_steps": self.total_steps,
            "percentage": (self.current_step / self.total_steps) * 100,
            "current_message": self.steps[min(self.current_step, len(self.steps) - 1)],
            "elapsed_time": elapsed,
            "detailed_logs": self.detailed_logs[-5:],
            "is_complete": self.current_step >= self.total_steps,
            "start_time": self.start_time,
            "timestamp": datetime.now().isoformat()
        }
    
    def _save_to_disk(self):
        """Salva estado do progresso no disco para recupera√ß√£o em caso de falha"""
        progress_dir = Path("analyses_data/progress")
        progress_dir.mkdir(parents=True, exist_ok=True)
        
        progress_file = progress_dir / f"progress_{self.session_id}.json"
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(self.get_current_status(), f, ensure_ascii=False, indent=2)

# Dicion√°rio para rastreamento de progresso local
local_progress_sessions = {}

def get_progress_tracker(session_id: str) -> LocalProgressTracker:
    """Obt√©m tracker de progresso local com recupera√ß√£o de estado"""
    if session_id in local_progress_sessions:
        return local_progress_sessions[session_id]
    
    # Tenta recuperar do disco se existir
    progress_dir = Path("analyses_data/progress")
    progress_file = progress_dir / f"progress_{session_id}.json"
    
    if progress_file.exists():
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
            
            tracker = LocalProgressTracker(session_id)
            tracker.current_step = progress_data.get('current_step', 0)
            tracker.start_time = progress_data.get('start_time', time.time())
            tracker.detailed_logs = progress_data.get('detailed_logs', [])
            local_progress_sessions[session_id] = tracker
            return tracker
        except Exception as e:
            logger.warning(f"Erro ao recuperar progresso do disco: {e}")
    
    # Cria novo tracker
    tracker = LocalProgressTracker(session_id)
    local_progress_sessions[session_id] = tracker
    return tracker

def update_analysis_progress(session_id: str, step: int, message: str, details: str = None):
    """Fun√ß√£o helper para atualizar progresso localmente"""
    tracker = get_progress_tracker(session_id)
    return tracker.update_progress(step, message, details)

@analysis_bp.route('/execute_complete_analysis', methods=['POST'])
def execute_complete_analysis():
    """Executa an√°lise completa com nova metodologia aprimorada ULTRA ROBUSTA"""
    start_time = time.time()
    session_id = None
    
    try:
        # Recebe dados da requisi√ß√£o
        data = request.get_json()
        if not data:
            return jsonify({"error": "Dados da requisi√ß√£o s√£o obrigat√≥rios"}), 400
        
        # Gera session_id √∫nico
        session_id = f"session_{int(time.time() * 1000)}_{uuid.uuid4().hex[:8]}"
        
        # Extrai par√¢metros
        segmento = data.get('segmento', '').strip()
        produto = data.get('produto', '').strip()
        publico = data.get('publico', '').strip()
        objetivos = data.get('objetivos', '').strip()
        contexto_adicional = data.get('contexto_adicional', '').strip()
        
        # Valida√ß√£o b√°sica
        if not segmento and not produto:
            return jsonify({"error": "Segmento ou produto s√£o obrigat√≥rios"}), 400
        
        # Constr√≥i query de pesquisa
        query_parts = []
        if segmento:
            query_parts.append(segmento)
        if produto:
            query_parts.append(produto)
        query_parts.append("Brasil 2024")
        
        query = " ".join(query_parts)
        
        # Contexto da an√°lise
        context = {
            "segmento": segmento,
            "produto": produto,
            "publico": publico,
            "objetivos": objetivos,
            "contexto_adicional": contexto_adicional,
            "methodology": "ARQV30_Enhanced_v4.0_REAL_DATA_ONLY"
        }
        
        # Salva dados da requisi√ß√£o
        requisicao_data = {
            "session_id": session_id,
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "context": context,
            "methodology": "REAL_DATA_v4.0"
        }
        
        auto_save_manager.salvar_etapa("requisicao_analise_aprimorada", requisicao_data, categoria="analise_completa")
        
        # Inicializa progresso local
        progress_tracker = get_progress_tracker(session_id)
        
        def progress_callback(step: int, message: str, details: str = None):
            """Callback para atualiza√ß√µes de progresso com recupera√ß√£o autom√°tica"""
            update_analysis_progress(session_id, step, message, details)
            # Salva progresso tamb√©m
            auto_save_manager.salvar_etapa("progresso", {
                "step": step,
                "message": message,
                "details": details
            }, categoria="logs")
        
        # Executa an√°lise completa com nova metodologia
        logger.info(f"üöÄ Iniciando an√°lise ULTRA ROBUSTA para session {session_id}")
        logger.info(f"üìã Query: {query}")
        logger.info(f"üéØ Segmento: {segmento} | Produto: {produto}")
        
        # Executa an√°lise com novo sistema aprimorado
        def execute_enhanced_analysis():
            try:
                import asyncio
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                try:
                    # ETAPA 1: Busca massiva real com valida√ß√£o rigorosa
                    progress_callback(1, "üåä Executando busca massiva real...")
                    search_results = loop.run_until_complete(
                        real_search_orchestrator.execute_massive_real_search(
                            query=query,
                            context=context,
                            session_id=session_id
                        )
                    )
                    
                    # Valida se os resultados s√£o REAIS
                    if not analysis_manager.validate_real_data(search_results, 'pesquisa_web'):
                        raise ValueError("Resultados de pesquisa cont√™m dados simulados ou fict√≠cios")
                    
                    # ETAPA 2: An√°lise de conte√∫do viral
                    progress_callback(2, "üî• Analisando conte√∫do viral...")
                    viral_analysis = loop.run_until_complete(
                        viral_content_analyzer.analyze_and_capture_viral_content(
                            search_results=search_results,
                            session_id=session_id
                        )
                    )
                    
                    # ETAPA 3: S√≠ntese com IA ativa
                    progress_callback(3, "üß† Executando s√≠ntese com IA...")
                    synthesis_result = loop.run_until_complete(
                        enhanced_synthesis_engine.execute_enhanced_synthesis(session_id)
                    )
                    
                    # ETAPA 3.5: An√°lise Preditiva Ultra-Avan√ßada
                    progress_callback(3.5, "üîÆ Executando an√°lise preditiva ultra-avan√ßada...")
                    predictive_insights = loop.run_until_complete(
                        predictive_analytics_engine.analyze_session_data(session_id)
                    )
                    
                    # ETAPA 4: Gera√ß√£o de m√≥dulos com garantia de integridade
                    progress_callback(4, "üìù Gerando 16 m√≥dulos com garantia de integridade...")
                    modules_result = loop.run_until_complete(
                        enhanced_module_processor.generate_all_modules(session_id)
                    )
                    
                    # Valida se todos os m√≥dulos obrigat√≥rios foram gerados
                    final_analysis = analysis_manager.ensure_all_modules_in_final_analysis(session_id)
                    if not final_analysis['_meta']['all_modules_included']:
                        missing_modules = final_analysis['_meta']['missing_required_modules']
                        raise ValueError(f"M√≥dulos obrigat√≥rios ausentes: {', '.join(missing_modules)}")
                    
                finally:
                    loop.close()
                
                return {
                    "success": True,
                    "search_results": search_results,
                    "viral_analysis": viral_analysis,
                    "synthesis_result": synthesis_result,
                    "predictive_insights": predictive_insights,
                    "modules_result": modules_result,
                    "phases_completed": ["busca_massiva", "analise_viral", "sintese_ia", "analise_preditiva", "geracao_modulos"],
                    "completeness_score": final_analysis['_meta']['completeness_percentage'],
                    "recommendations": analysis_manager._generate_recommendations(
                        analysis_manager.get_comprehensive_session_summary(session_id)['modules_analysis']
                    )
                }
                
            except Exception as e:
                logger.error(f"‚ùå Erro na an√°lise aprimorada: {e}")
                return {
                    "success": False,
                    "error": str(e),
                    "error_type": type(e).__name__
                }
        
        # Executa an√°lise
        analysis_results = execute_enhanced_analysis()
        
        # Finaliza progress tracker
        progress_tracker.complete()
        
        # Resposta da API
        if analysis_results.get("success"):
            # Calcula tempo de execu√ß√£o
            execution_time = time.time() - start_time
            
            # Obt√©m resumo completo da sess√£o
            session_summary = analysis_manager.get_comprehensive_session_summary(session_id)
            
            response = {
                "success": True,
                "session_id": session_id,
                "methodology": "ARQV30_Enhanced_v4.0_REAL_DATA_ONLY",
                "message": "An√°lise completa conclu√≠da com sucesso",
                "execution_summary": {
                    "execution_time_seconds": execution_time,
                    "execution_time_formatted": f"{int(execution_time // 60)}m {int(execution_time % 60)}s",
                    "phases_completed": analysis_results.get("phases_completed", []),
                    "total_sources": analysis_results.get("search_results", {}).get("statistics", {}).get("total_sources", 0),
                    "viral_content": len(analysis_results.get("viral_analysis", {}).get("viral_content_identified", [])),
                    "screenshots_captured": len(analysis_results.get("viral_analysis", {}).get("screenshots_captured", [])),
                    "modules_generated": analysis_results.get("modules_result", {}).get("successful_modules", 0),
                    "completeness_score": session_summary['completeness_score'],
                    "recommendations": session_summary['recommendations']
                },
                "data_quality": {
                    "sources_quality": "PREMIUM - 100% dados reais",
                    "processing_quality": "ULTRA_HIGH",
                    "viral_content_captured": True,
                    "ai_active_search": True,
                    "api_rotation_used": True,
                    "zero_simulated_data": True
                },
                "session_summary": {
                    "completeness_percentage": session_summary['completeness_score'],
                    "required_modules_completed": len([m for m in session_summary['modules_analysis'].values() 
                                                     if m['required'] and m['has_data']]),
                    "total_required_modules": len([m for m in session_summary['modules_analysis'].values() 
                                                 if m['required']]),
                    "modules_with_high_quality": len([m for m in session_summary['modules_analysis'].values() 
                                                    if m['has_data'] and m['quality_score'] >= 80]),
                    "recommendations": session_summary['recommendations']
                },
                "access_info": {
                    "session_directory": f"analyses_data/{session_id}",
                    "screenshots_directory": f"analyses_data/files/{session_id}",
                    "modules_directory": f"analyses_data/{session_id}/modules",
                    "final_report_available": True,
                    "progress_tracking": "local"
                },
                "timestamp": datetime.now().isoformat()
            }
            
            # Salva resposta final
            auto_save_manager.salvar_etapa("resposta_final", response, categoria="analise_completa")
            
            logger.info(f"‚úÖ An√°lise ULTRA ROBUSTA conclu√≠da com sucesso: {session_id}")
            return jsonify(response), 200
        
        else:
            # Tenta recuperar dados salvos automaticamente
            try:
                dados_recuperados = auto_save_manager.consolidar_sessao(session_id)
                logger.info(f"üîÑ Dados recuperados automaticamente: {dados_recuperados}")
                
                error_response = {
                    "success": False,
                    "session_id": session_id,
                    "methodology": "ARQV30_Enhanced_v4.0_REAL_DATA_ONLY",
                    "error": analysis_results.get("error", "Erro desconhecido"),
                    "error_type": analysis_results.get("error_type", "UnknownError"),
                    "message": "An√°lise falhou durante execu√ß√£o - dados parciais recuperados",
                    "dados_recuperados": True,
                    "session_summary": analysis_manager.get_comprehensive_session_summary(session_id),
                    "timestamp": datetime.now().isoformat(),
                    "recommendation": "Verifique os dados recuperados e tente novamente"
                }
                
                logger.warning(f"‚ö†Ô∏è An√°lise falhou mas dados parciais foram recuperados: {session_id}")
                return jsonify(error_response), 206  # Partial Content
            
            except Exception as recovery_error:
                logger.error(f"‚ùå Falha na recupera√ß√£o autom√°tica: {recovery_error}")
                
                error_response = {
                    "success": False,
                    "session_id": session_id,
                    "methodology": "ARQV30_Enhanced_v4.0_REAL_DATA_ONLY",
                    "error": analysis_results.get("error", "Erro desconhecido"),
                    "error_type": analysis_results.get("error_type", "UnknownError"),
                    "message": "An√°lise falhou durante execu√ß√£o - sem dados recuper√°veis",
                    "dados_recuperados": False,
                    "timestamp": datetime.now().isoformat(),
                    "recommendation": "Verifique configura√ß√£o das APIs e tente novamente",
                    "debug_info": {
                        "input_data": {
                            "segmento": segmento,
                            "produto": produto,
                            "query": query
                        }
                    }
                }
                
                logger.error(f"‚ùå An√°lise falhou sem recupera√ß√£o poss√≠vel: {session_id} - {analysis_results.get('error')}")
                return jsonify(error_response), 500
    
    except Exception as e:
        logger.exception(f"‚ùå Erro cr√≠tico na rota de an√°lise: {e}")
        
        # Remove progresso local em caso de erro
        if session_id and session_id in local_progress_sessions:
            del local_progress_sessions[session_id]
        
        return jsonify({
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__,
            "message": "Erro interno do servidor",
            "session_id": session_id if session_id else "unknown",
            "timestamp": datetime.now().isoformat(),
            "recommendation": "Contate o suporte t√©cnico com o ID da sess√£o"
        }), 500

@analysis_bp.route('/progress/<session_id>', methods=['GET'])
def get_analysis_progress(session_id):
    """Obt√©m progresso da an√°lise com recupera√ß√£o robusta"""
    try:
        # Tenta obter do sistema local primeiro
        if session_id in local_progress_sessions:
            tracker = local_progress_sessions[session_id]
            return jsonify({
                "success": True,
                "progress": tracker.get_current_status(),
                "system": "local_tracker"
            })
        
        # Tenta obter do progress tracker padr√£o
        progress_info = progress_tracker_enhanced.get_session_progress(session_id)
        if progress_info:
            return jsonify({
                "success": True,
                "progress": progress_info,
                "system": "enhanced_tracker"
            })
        
        # Tenta recuperar do disco
        progress_dir = Path("analyses_data/progress")
        progress_file = progress_dir / f"progress_{session_id}.json"
        
        if progress_file.exists():
            try:
                with open(progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                return jsonify({
                    "success": True,
                    "progress": progress_data,
                    "system": "disk_recovery"
                })
            except Exception as e:
                logger.error(f"Erro ao ler progresso do disco: {e}")
        
        # Verifica se a an√°lise est√° completa
        summary = analysis_manager.get_comprehensive_session_summary(session_id)
        if summary['completeness_score'] > 95:
            return jsonify({
                "success": True,
                "progress": {
                    "session_id": session_id,
                    "current_step": 13,
                    "total_steps": 13,
                    "percentage": 100,
                    "current_message": "An√°lise conclu√≠da com sucesso",
                    "is_complete": True,
                    "completeness_score": summary['completeness_score']
                },
                "system": "session_summary"
            })
        
        return jsonify({
            "success": False,
            "error": "Sess√£o n√£o encontrada",
            "session_id": session_id,
            "message": "N√£o foi poss√≠vel localizar informa√ß√µes de progresso para esta sess√£o"
        }), 404
    
    except Exception as e:
        logger.error(f"‚ùå Erro ao obter progresso: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "session_id": session_id,
            "message": "Erro interno ao obter progresso"
        }), 500

@analysis_bp.route('/session_summary/<session_id>', methods=['GET'])
def get_session_summary(session_id):
    """Obt√©m resumo completo da sess√£o com valida√ß√£o de integridade"""
    try:
        # Obt√©m resumo completo
        summary = analysis_manager.get_comprehensive_session_summary(session_id)
        
        # Verifica se a sess√£o existe
        if not summary['modules_analysis']:
            return jsonify({
                "success": False,
                "error": "Sess√£o n√£o encontrada",
                "session_id": session_id,
                "message": "N√£o foram encontrados dados para esta sess√£o"
            }), 404
        
        # Prepara resposta
        response = {
            "success": True,
            "session_id": session_id,
            "summary": summary,
            "timestamp": datetime.now().isoformat(),
            "data_integrity": {
                "all_modules_present": summary['completeness_score'] >= 95,
                "zero_simulated_data": True,
                "real_data_validation": "100% validado"
            }
        }
        
        return jsonify(response), 200
    
    except Exception as e:
        logger.error(f"‚ùå Erro ao obter resumo da sess√£o: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "session_id": session_id,
            "message": "Erro interno ao obter resumo da sess√£o"
        }), 500

@analysis_bp.route('/validate_real_data', methods=['POST'])
def validate_real_data():
    """Valida se dados s√£o REAIS (n√£o simulados/fict√≠cios)"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "Dados n√£o fornecidos"}), 400
        
        category = data.get('category', 'generic')
        test_data = data.get('data')
        
        is_real = analysis_manager.validate_real_data(test_data, category)
        quality_score = analysis_manager.calculate_data_quality(test_data, category)
        
        return jsonify({
            "success": True,
            "is_real_data": is_real,
            "quality_score": quality_score,
            "category": category,
            "timestamp": datetime.now().isoformat()
        })
    
    except Exception as e:
        logger.error(f"‚ùå Erro na valida√ß√£o de dados reais: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "message": "Erro interno na valida√ß√£o de dados"
        }), 500

@analysis_bp.route('/system_status', methods=['GET'])
def get_system_status():
    """Retorna status completo do sistema com valida√ß√£o de integridade"""
    try:
        # Verifica integridade do sistema
        try:
            analysis_manager._validate_system_integrity()
            system_integrity = "healthy"
        except Exception as e:
            system_integrity = "degraded"
            logger.error(f"Sistema com integridade comprometida: {e}")
        
        # Status dos m√≥dulos
        modules_status = {
            name: {
                "status": "ok" if (analysis_manager.base_path / name).exists() else "missing",
                "required": info["required"],
                "priority": info["priority"]
            } for name, info in analysis_manager.required_modules.items()
        }
        
        # Status do progresso local
        local_progress_status = {
            "active_sessions": len(local_progress_sessions),
            "sessions": list(local_progress_sessions.keys())
        }
        
        # Prepara resposta
        response = {
            "system": "ARQV30_Enhanced_v4.0_ULTRA_ROBUSTO",
            "status": "operational" if system_integrity == "healthy" else "degraded",
            "system_integrity": system_integrity,
            "timestamp": datetime.now().isoformat(),
            "components": {
                "data_integrity": {
                    "status": "healthy" if system_integrity == "healthy" else "warning",
                    "message": "Todos os m√≥dulos obrigat√≥rios est√£o presentes" if system_integrity == "healthy" 
                              else "Alguns m√≥dulos obrigat√≥rios est√£o ausentes"
                },
                "progress_tracking": {
                    "status": "healthy",
                    "type": "hybrid (local + enhanced)",
                    "active_sessions": len(local_progress_sessions)
                },
                "real_data_validation": {
                    "status": "healthy",
                    "validation_system": "active",
                    "zero_simulated_data": True
                },
                "modules": modules_status
            },
            "capabilities": {
                "real_data_only": True,
                "zero_simulated_content": True,
                "auto_recovery": True,
                "local_progress_tracking": True,
                "comprehensive_session_summary": True,
                "integrity_validation": True
            }
        }
        
        return jsonify(response), 200
    
    except Exception as e:
        logger.exception(f"‚ùå Erro ao obter status do sistema: {e}")
        return jsonify({
            "error": "Erro interno",
            "message": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

# Rota de compatibilidade
@analysis_bp.route('/analyze', methods=['POST'])
def analyze_compatibility():
    """Rota de compatibilidade que redireciona para nova metodologia ultra robusta"""
    logger.info("üîÑ Redirecionando para nova metodologia ULTRA ROBUSTA")
    return execute_complete_analysis()

# Rota para resetar sistema (para depura√ß√£o)
@analysis_bp.route('/reset_system', methods=['POST'])
def reset_system():
    """Reseta o sistema para estado inicial com verifica√ß√£o de integridade"""
    try:
        # Limpa progresso local
        local_progress_sessions.clear()
        
        # Re-inicializa o manager de an√°lise
        global analysis_manager
        analysis_manager = UltraRobustAnalysisManager()
        
        # Reseta progress tracker
        progress_tracker_enhanced.reset()
        
        logger.info("üîÑ Sistema resetado com sucesso")
        
        return jsonify({
            "success": True,
            "message": "Sistema resetado com sucesso",
            "system_status": get_system_status().get_json(),
            "timestamp": datetime.now().isoformat()
        }), 200
    
    except Exception as e:
        logger.error(f"‚ùå Erro ao resetar sistema: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "message": "Erro ao resetar sistema"
        }), 500